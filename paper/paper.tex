% TEMPLATE for Usenix papers, specifically to meet requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk.  Complaints to /dev/null.
% make it two column with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template, so that people can
% more easily include the .sty file into an existing document.  Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file. 

% Note that since 2010, USENIX does  not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the 
% usepackage command, below.

% This version uses the latex2e styles, not the very ancient 2.09 stuff.
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes}

\usepackage{algpseudocode}

\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\begin{document}

%don't want date printed
\date{\today}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf New Static Analysis Techniques to Detect Entropy Failure Vulnerabilities}

%for single author (just remove % characters)
\author{
{\rm Rushi Shah}
\and
{\rm Andrew Russell}
}

\maketitle

\section{Introduction}

One common misuse of cryptography is the misuse of entropy. Without proper random inputs, many cryptographic algorithms
are vulnerable to basic forms of cryptanalysis. Some cryptographic schemes, such as DSA, can even disclose long-term secrets
such as the signing key when the random per-message input is low entropy, made public, or nonunique.

We plan to use data dependency tools to determine how entropic inputs in a given program are used by various cryptographic algorithms. That will allow us to identify if and when entropy is too low or is misused. We plan to produce this as a code integration tool for developers to use as part of a compiler toolchain.
Our tool will seek to generate an error report from source code using notions of taint analysis. 

For this project we consider codebases that maintain version history, in the hopes that we can employ static analysis techniques across program versions to infer more information about how entropy is being used; 
our primary goal is to identify bugs that are introduced to existing codebases. This may prove to be especially helpful since naive taint analysis can provide mere overapproximations of data dependency for a given program and may generate many false alarms for programmers in a professional development setting.

\section{Preliminaries}

\subsection{Taint Analysis}

\subsection{Predicate Abstraction}

\subsection{$k$-safety properties}

\subsection{Product Programs}

\section{Our Approach}

\subsection{Instrumenting Programs for Predicate Abstraction}

\subsection{Naive Product Program Inference Rules}

\subsection{Taint-augmented Inference Rules}

\subsection{Putting it all together}

\section{Results}

\section{Conclusion}

\section{Future Work}

\section{Considered Approaches}

\subsection{Taint Analysis}
Static code analysis has a history of identifying security vulnerabilities at a source code level. Some examples include SQL injection, cross-site scripting exploits, and buffer overflow attacks. However, there has not been any attempt in the literature to statically analyze source code for cryptographic vulnerabilities stemming from entropy misuse, which we seek to do. We claim that standard static code analysis techniques developed thus far are insufficient for the analysis we would like to perform. 
The sources of entropy can be tainted standard taint analysis techniques can be used to approximate how the taint propagates. But this approximation may be too coarse to provide useful information to the user. Even with a sufficiently precise taint analysis, it is unclear how to determine the ``correct'' set of sources a cryptographic value should rely on at any point in the program.

\subsection{Differential Taint Analysis}

Since we want a stronger notion of taint analysis than a naive approach, we intend to introduce and formalize the concept of ``differential taint analysis." Differential
taint analysis, in the ideal, will give us more information than simple taint analysis by leveraging 
the original version of the program as a source of ground truth. This additional information could include reducing programmer annotation work or reduce false positives between versions
(since taint analysis overapproximates data flow). By viewing the original version of the program as bug-free, we reduce the number of code audits required:
only the original version of the code would need to be audited, and future versions could rely on our tool to automatically detect bugs that were introduced by refactors.

\subsection{Product Programs}

One technique that we have considered toward the construction of differential taint analysis is the notion of a ``product program.'' In the most general sense, the product $P_1 \times P_2$ of two programs $P_1$ and $P_2$ is used to verify
relations between the programs, such as equivalence. Product programs have also been used to analyze different runs of the same program. 
The product program $P_1 \times P_2$ is semantically equivalent to the sequential composition $P_1; P_2$, but such that we can prove useful safety properties of $P_1 \times P_2$ that would be difficult to prove with standard techniques on $P_1$, $P_2$, or $P_1; P_2$.

In our setting, we are not so much interested in proving safety properties of a program $P$ but instead wish to prove that cryptographic values rely on sufficiently many bits of entropy when they are used via taint analysis, 
which is why we cannot simply leverage existing off-the-shelf product program analysis techniques.

\subsubsection{Example}

Consider the following two programs $P_1$ and $P_2$ that are equivalent from a cryptographic point of view, but might have small feature changes. 

\begin{algorithmic}
\Function{$P_1$}{}
\State $s \gets$ entropy
\State $\vdots$
\If {$p$}
\State $c \gets$ AES(s)
\EndIf
\State $\vdots$
\EndFunction
\\
\Function{$P_2$}{}
\State $s \gets$ entropy
\State $\vdots$
\If {$p$}
\State $c \gets$ AES(s)
\EndIf
\State $\vdots$
\EndFunction
\end{algorithmic}

where $p$ is some complex predicate that is difficult to reason about, but does not change between $P_1$ and $P_2$ (and no values that $p$ depends on change, either). Then, a safety property about the entropy of the cryptographic values in $c$ would hold in both programs or in neither program. 

\subsection{Verification Modulo Versions}
Verification modulo versions is an idea quite similar to product programs. VMV attempts to identify abstract regressions and relative correctness between two versions of a program.
In VMV, the static analyzer is treated as opaque in an effort to cut down the number of alarms by inferring assumptions made by the original program (assumed to be correct). This is especially appealing in the security software development cycle because there are documented cases of entropy bugs being introduced in new versions of programs (for example, OpenSSL and FreeBSD). This would involve under-approximating the taint of one version of the program and over-approximating the taint of another version of the program. Current tooling does not widely support under-approximating taint analysis. 


\subsection{Relational Verification}
Another existing technique is relational verification, in which relational properties between two programs are used to prove $k$-safety properties. A $k$-safety property is a safety property of a program that requires 
reasoning about the relationships between $k$ different runs of a program (or $k$ programs). For example, transitivity is a $k$-safety property.
In our case, we would want to show the equivalence of entropy assignment and the equivalence of the entropy propagation. Relational verification utilitzes product programs; RV constructs a product program between versions of the program that captures the semantics of executing them sequentially, but that lends itself to standard verification techniques. 

\subsection{Differential Assertion Checking}

The last approach that we have considered is differential assertion checking. Differential assertion checking seeks to (statically) prove the relative correctness between two similar programs,
with a significantly lower cost than ensuring absolute correctness. That is, DAC allows is to answer if there are conditions under which a program $P_1$ passes an assertion check but
a program $P_2$ fails. The techniques utilized current DAC work may be of interest, but it seems they rely on an a priori mapping between the two programs, which is something that
relational verification work seeks to answer. Thus DAC will be considered further after the idea of product programs is exhausted.

% How is vmv related to relational verification? 


% Standard taint analysis will under/overapproximate the taint, so we may not be able to use it to directly prove our safety property on the target program. Thus, we can leverage the approach verification modulo versions to prove relational verification. 

% Relational verification aims to solve this problem by proving relational properties between two programs. However, the naive relational verification approach of verifying each program individually is infeasible given arbitrarily difficult-to-reason-about functions even if those functions do not change between versions of the program. One approach is to underapproximate the taint on one program and overapproximate the taint on another program to verify the set equality of taint sources, but this still falls prey to the approximation of taint analysis given arbitrarily complex functions. Also, current tooling does not widely support underapproximating taint analysis.

% Finally, even with a sufficiently precise taint analysis, it is unclear how to determine the ``correct'' set of sources a cryptographic value should rely on at any point in the program. For example, some programs will use weak entropy on system startup, and increase the entropy in those values, later on. Thus, by running our analysis on two versions of the program, we can use one version as an oracle for the correctness criteria of the other program. In other words, we can use it to infer what the set of taint results should be for any variable at any point in the program.  

\section{Our Approach}
Given the two programs in the example, we propose generating a product program similar to the following:

\begin{algorithmic}
\Function{$P_1 \times P_2$}{}
\State $S_1 \gets$ entropy
\State $S_2 \gets$ entropy
\State $\vdots$
\If {f()}
\State c$_1 \gets$ AES($s_1$)
\State $c_2 \gets$ AES($s_2$)
\EndIf
\State $\vdots$
\EndFunction
\end{algorithmic}

Note that, during the construction of the product program, we need to be able to merge the if statements in the two programs. 

We would like to demonstrate set equality between the taint for $s_1, s_2$ when they are used in the if statement, which amounts to demonstrating the equivalence of the taint propagation through the program. We would also like to demonstrate that we assign the output of a sufficiently entropic value to a variable in $P_1$ if and only if we also assign it that sufficiently entropic value in $P_2$.

Proving these two properties (set-equality and equivalence of assignments) allows us to claim that $P_1$ has the safety property if and only if $P_2$ has the safety property.

\section{Real World Applications}

A verification tool like this could be especially useful for a maintainer of a project that relies on cryptographic values. For example, if she audits her code once to confirm that it uses entropy properly, she can use differential taint analysis on future commits to confirm that the new code does not introduce this class of bugs. % VMV?

Our tool should be able to detect the infamous Debian OpenSSL entropy bug. We will also test our tool on other libraries that rely on entropy, such as Amazon's signal2noise, LibreSSL, GnuTLS, and GnuPG,
which are not known to contain entropy bugs (and likely do not), but we can test on versions in which we introduce our own bugs.

\section{Research Hypotheses}

These are the principal hypotheses we would like to test:

\begin{enumerate}
	\item An automated tool can detect entropy bugs in real-world programs.
	\item Entropy is insufficiently propagated in programs that rely on cryptography, or entropy propagation follows nontrivial code paths (due to error handling or other control flow).
	\item Multiple versions of the same program can make static analysis for this domain more effective by lowering costs (computational and programmer) or providing more fine-grained information.
\end{enumerate}

\section{Future Work}

As bad data flow can be a source of security bugs other than simply (lack of) entropy propagation (for example, 
address disclosure and use-after-free bugs in browsers), we believe that our tool could also be applied to these settings to help prevent
programmers from introducing security vulnerabilities into their projects.

\section{Links}

\begin{enumerate}
	\item Debian/OpenSSL Bug 
		\begin{enumerate}
			\item \url{https://www.schneier.com/blog/archives/2008/05/random_number_b.html}
			\item \url{https://research.swtch.com/openssl}
			\item \url{https://freedom-to-tinker.com/2013/09/20/software-transparency-debian-openssl-bug/}
			\item \url{https://www.cs.umd.edu/class/fall2017/cmsc818O/papers/private-keys-public.pdf}
		\end{enumerate}
	\item Data flow
		\begin{enumerate}
			\item \url{https://en.wikipedia.org/wiki/Data-flow_analysis}
			\item \url{https://www.seas.harvard.edu/courses/cs252/2011sp/slides/Lec02-Dataflow.pdf}
		\end{enumerate}
	\item Static Program Analysis
		\begin{enumerate}
            \item \url{https://cs.au.dk/~amoeller/spa/spa.pdf}
            \item \url{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859783}
		\end{enumerate}
	\item Relational Verification:
		\begin{enumerate}
			\item \url{https://dl.acm.org/citation.cfm?id=2021319}
			\item \url{https://ac.els-cdn.com/S235222081630044X/1-s2.0-S235222081630044X-main.pdf?_tid=076a0492-9cee-4995-9710-bcb3c64b98e0&acdnat=1539815890_178849b4f14af3751e9acb03b238db4d}
			\item \url{https://www.microsoft.com/en-us/research/publication/differential-assertion-checking/}
			\item \url{https://www.microsoft.com/en-us/research/wp-content/uploads/2014/06/paper-1.pdf}
			\item \url{https://www.cs.utexas.edu/~isil/pldi16-chl.pdf}
		\end{enumerate}
	\item Projects to analyze 
		\begin{enumerate}
			\item OpenPGP
			\item BouncyCastle
			\item OpenSSL
			\item GnuPGP
			\item F\# SSL project with proof of correctness
			\item NQSBTLS
			\item Amazon's s2n (signal to noise)
		\end{enumerate}
\end{enumerate}

% We have two motivating examples. First, we would like our tool to be able to detect the issue with the Debian/OpenSSL pseudo-random number generator that was exposed in 2008. 
% Second, we would like to identify potential vulnerabilities in current cryptocurrency wallet code as many cryptocurrency protocols rely on DSA.

% \subsection{Goals}
% We plan to use data dependency tools to determine how entropic inputs in a given program are used by various cryptographic algorithms. 
% That will allow us to identify if and when entropy is too low or is misused. We plan to either produce
% this as a code integration tool for developers to use as part of a compiler toolchain, or use this tool to analyze a large number of codebases
% found ``in the wild,'' such as those written by amateurs.

% Our tool will seek to generate a human-checkable dependency graph from source code
% using taint analysis on functions and function inputs manually specified via annotation by the developer.
% This graph will allow the developer to manually verify that entropy is used properly; a stretch goal 
% of ours would be to automate the identification of a subset of known misuses of entropy.

\end{document}
